{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waveforms to Xarray\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Warning**: This part of obsplus is still very experimental and subject to rapid changes, proceed with caution.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "The [xarray library](http://xarray.pydata.org/en/stable/) offers pandas-like data structures that are not limited to 2 dimensions (rows and columns). We have found working with seismic waveform data in such a way can be useful, but certainly is not as general as [obspy streams](https://docs.obspy.org/packages/autogen/obspy.core.stream.Stream.html). Particularly, Xarray data structures don't work well with gappy data or data with non-uniform sampling rates.\n",
    "\n",
    "Before attempting to use these features in obsplus we highly recommend you read through the [xarray documentation](http://xarray.pydata.org/en/stable/) as the API may take a bit of time to learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating data arrays\n",
    "\n",
    "Creating DataArrays from ObsPy objects is straight-forward. A single `Trace`, `Stream`, or collection (list-like) of either, or a mapping (dict-like) of either are all valid inputs. If a mapping is used, they keys will often correspond to an event id.\n",
    "\n",
    "Conceptually the DataArray looks like this:\n",
    "\n",
    "<img src=\"../../images/data_array2.png\" width=\"350\" height=\"350\" align=\"left\"/>\n",
    "<br clear=\"all\">\n",
    "\n",
    "Each `DataArray` instance created by ObsPlus has three dimensions:\n",
    "\n",
    "1. __stream_id__: The keys used in the dictionary or an integer starting at 0.\n",
    "\n",
    "2. __seed_id__: The seed id (ie network.station.location.channel) of each trace.\n",
    "\n",
    "3. __time__: Floating point values beginning at zero and incrementing by the sampling period.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import obspy\n",
    "import obsplus\n",
    "from obsplus import obspy_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = obspy.read()\n",
    "# create data array from a trace\n",
    "from_trace = obsplus.obspy_to_array(st[0])\n",
    "# create data array from stream\n",
    "from_stream = obsplus.obspy_to_array(st)\n",
    "# create a data array from a list of streams\n",
    "st_list = [st.copy() for _ in range(3)]\n",
    "from_list = obsplus.obspy_to_array(st_list)\n",
    "# create data array from a dict of streams\n",
    "st_dict = {f'event{x}': st.copy() for x in range(3)}\n",
    "from_dict = obsplus.obspy_to_array(st_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(from_trace.dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data array created from a single trace will only have a size of one in the stream_id and seed_id columns, and the time dimension will be as long as the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(from_trace.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataarray created from a dict of streams, however, will be larger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(from_dict.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The xarray `str` representation is fairly large, but very descriptive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print value for the seed_id dimension\n",
    "print(from_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataArrays can also be converted back to dictionaries of `Stream` objects. The transformation *should* be lossless:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(from_dict.ops.to_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of the DataArray\n",
    "\n",
    "The DataArray has two potential advantages over the Stream representation:\n",
    "\n",
    "   1. Efficiency \n",
    "    \n",
    "   2. Organization\n",
    "\n",
    "### Efficiency\n",
    "\n",
    "There have been many efforts to improve efficiency of numpy/scipy functionality. Some of these, such as [Intel's MKL](https://software.intel.com/en-us/mkl) ship with scientific python distributions, like [Anaconda](https://www.anaconda.com/). These optimizations are great because you don't need to change anything about your code; it just runs faster.\n",
    "\n",
    "Some of these optimizations involve making better use of modern hardware, particularly processors with many cores. It is much better to let the well-tested low-level libraries handle parallelism rather than implementing messy multiprocessing/multithreading python code when possible. For example, let's compare the time required to calculating FFTs for each `Trace` in a large `Stream` vs doing it all at once on a `DataArray` created with ObsPlus, both of which should return the same result. The latter will be more efficient because it allows numpy to better plan optimization strategies, as well as avoids python loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "print(f'numpy version: {np.__version__}')\n",
    "print(f'xarray version: {xr.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test streams with random data\n",
    "import numpy as np\n",
    "import obspy\n",
    "\n",
    "\n",
    "num_stations = 200\n",
    "sr = 100\n",
    "data_length = sr * 60\n",
    "\n",
    "traces = []\n",
    "for station in ('{x:03d}'.format(x=x) for x in range(num_stations)):\n",
    "    data = np.random.rand(data_length)\n",
    "    stats = dict(network='OP', station=station, location='', channel='HHZ',\n",
    "                 sampling_rate=sr)\n",
    "    traces.append(obspy.Trace(data=data, header=stats))\n",
    "\n",
    "st = obspy.Stream(traces=traces)\n",
    "print(st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to data array\n",
    "dar = obspy_to_array(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Time looping through traces and performing fft\n",
    "out1 = np.array([np.fft.rfft(tr.data) for tr in st])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Time performing fft in one-go on large numpy block\n",
    "out2 = np.fft.rfft(dar.data, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after flattening one dimension in out2, the results should be (nearly) the same\n",
    "assert np.allclose(out1, out2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The xarray version is usually between 2 and 20 times faster, depending on the number of cores in your CPU and the python distribution you are using. However, this notebook may not show much of a difference if it was executed on the ReadTheDocs server. The best way to assess performance gains is to download and run this notebook yourself.\n",
    "\n",
    "Moreover xarray provides ways of easily working with dask for distributed computing. This would be a bit more difficult using `Stream`s. See [this](http://xarray.pydata.org/en/stable/dask.html) for more details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organization\n",
    "\n",
    "With the data organized in a 3D cube of sorts, it becomes fairly natural to slice and manipulate the data because xarray, like pandas, has intuitive and efficient indexing and sensible broadcasting. Here are a few examples of what you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get middle 10 seconds of data\n",
    "time_mean = dar.time.mean()\n",
    "duration = dar.time.max() - dar.time.min()\n",
    "dar.sel(time=slice(time_mean - 5, time_mean + 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim seed_id (channels) to only include data from every 13th station\n",
    "stations = list('OP.{x:03d}..HHZ'.format(x=x) for x in range(0, data_length, 13))\n",
    "dar.where(dar.seed_id.isin(stations), drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple detrend using mean\n",
    "dar - dar.mean(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rolling sta/lta on each channel\n",
    "sta_samples = 25\n",
    "lta_samples = 200\n",
    "\n",
    "sta = dar.rolling(time=sta_samples).mean()\n",
    "lta = dar.rolling(time=lta_samples).mean()\n",
    "\n",
    "result = sta / lta\n",
    "print(result.dropna(dim='time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obsplus Accessor methods\n",
    "Obsplus registers an [xarray accessor](http://xarray.pydata.org/en/stable/internals.html#extending-xarray) to add seismic specific functionality. These are accessed via the `ops` attribute which is available as long as obsplus is imported. Here is a brief tour of some of the methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a data array from the bingham dataset\n",
    "import obsplus\n",
    "fetcher = obsplus.load_dataset('bingham').get_fetcher()\n",
    "# init dict of {event_id: stream} and get catalog/inventory\n",
    "st_dict = dict(fetcher.yield_event_waveforms(time_before=5, time_after=30))\n",
    "cat = fetcher.event_client.get_events()\n",
    "inv = fetcher.station_client.get_stations()\n",
    "# create datarray\n",
    "dar = obsplus.obspy_to_array(st_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select channels based on seed_ids (wildcards permitted)\n",
    "filtered_dar = dar.ops.sel_sid('UU.*.*.ENZ')\n",
    "\n",
    "# filter check\n",
    "assert len(filtered_dar.seed_id)\n",
    "for seed_id in filtered_dar.seed_id.values:\n",
    "    assert seed_id.endswith('ENZ')\n",
    "    assert seed_id.startswith('UU')\n",
    "\n",
    "print(filtered_dar.seed_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcule rffts (note the \"time\" dimension has changed to \"frequency\")\n",
    "rfft = dar.ops.rfft()\n",
    "# print dimensions and corresponding size\n",
    "print(dict(zip(rfft.dims, rfft.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate irffts\n",
    "irfft = rfft.ops.irfft()\n",
    "# print dimensions and corresponding size\n",
    "print(dict(zip(irfft.dims, irfft.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back into a dict of streams\n",
    "st_dict = dar.ops.to_stream()\n",
    "for event_id, st in st_dict.items():\n",
    "    print(f'event_id: {event_id} stream_size: {len(st)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach event information\n",
    "dar_with_events = dar.ops.attach_events(cat)\n",
    "# note the extra coords that have now been attached\n",
    "print(dar_with_events.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim all waveforms to the mean of the P times picked on the same station\n",
    "# if no such P pick exists the channels will not be trimmed\n",
    "out = dar_with_events.ops.trim('p_time', aggregate_by='station')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over slices of stations\n",
    "for seed_dar in dar.ops.iter_seed('station'):\n",
    "    print(f'got channels: {set(seed_dar.seed_id.values)} in yielded data array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the norm of the data recorded at each station\n",
    "from numpy.linalg import norm\n",
    "dar.ops.agg(np.linalg.norm, level='station')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
