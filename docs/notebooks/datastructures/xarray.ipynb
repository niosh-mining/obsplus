{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waveforms to Xarray\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Warning**: This part of obsplus is still very experimental and subject to rapid changes, proceed with caution.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "The [xarray library](http://xarray.pydata.org/en/stable/) offers pandas-like data structures that are not limited to 2 dimensions (rows and columns). We have found working with seismic waveform data in such a way can be useful, but certainly is not as general as [obspy streams](https://docs.obspy.org/packages/autogen/obspy.core.stream.Stream.html). Particularly, Xarray datastructures dont work well with gappy data or data that don't have uniform sampling rates.\n",
    "\n",
    "If your data have few gaps and are uniform in sampling rate (eg array processing), this module may be useful to you. Xarray is also a first class citizen in distributed computing work being done in [Dask](https://dask.org/) so many of the advances being made may be more accessible to seismic time-series data when this structure is used. However, we have not yet used these features ourselves.\n",
    "\n",
    "Before attempting to use these features in obsplus we highly recommend you read through the [xarray documentation](http://xarray.pydata.org/en/stable/) as the API may take a bit of time to learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating data arrays\n",
    "\n",
    "Creating DataArrays from obspy objects is straight-forward. A single trace, a stream, a collection (list-like) of streams, or a mapping (dict-like) of streams are all valid inputs. If a mapping is used, they keys will often correspond to an event id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import obspy\n",
    "import obsplus\n",
    "from obsplus import obspy_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = obspy.read()\n",
    "# create data array from a trace\n",
    "from_trace = obsplus.obspy_to_array(st[0])\n",
    "# create data array from stream\n",
    "from_stream = obsplus.obspy_to_array(st)\n",
    "# create a data array from a list of streams\n",
    "st_list = [st.copy() for _ in range(3)]\n",
    "from_list = obsplus.obspy_to_array(st_list)\n",
    "# create data array from a dict of streams\n",
    "st_dict = {f'event{x}': st.copy() for x in range(3)}\n",
    "from_dict = obsplus.obspy_to_array(st_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each data array has three dimensions:\n",
    "\n",
    "1. __stream_id__: The keys used in the dictionary or an int starting at 0.\n",
    "\n",
    "2. __seed_id__: The seed id of each trace.\n",
    "\n",
    "3. __time__: Floating point values beginning at zero and incrementing by the sampling period for each trace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(from_trace.dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data array created from a single trace will only have a size of one in the stream_id and seed_id columns, and the time dimension will be as long as the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(from_trace.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataarray created from a dict of streams, however, will be larger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(from_dict.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Xarray `str` representation is fairly large, but very descriptive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print value for the seed_id dimension\n",
    "print(from_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataArrays can also be converted back to dicts `Stream` objects. The transformation *should* be lossless:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(from_dict.ops.to_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of the dataarray\n",
    "\n",
    "The DataArray has two potential advantages over the Stream representation:\n",
    "\n",
    "    1) Efficiency \n",
    "    \n",
    "    2) Organization\n",
    "\n",
    "### Efficiency\n",
    "\n",
    "There have been many efforts to improve efficiency of numpy/scipy functionality. Some of these, such as [Intel's MKL](https://software.intel.com/en-us/mkl) ship with scientific python distributions, like [Anaconda](https://www.anaconda.com/). These optimizations are great because you don't need to change anything about your code; it just runs faster.\n",
    "\n",
    "Some of these optimizations involve making better use of modern hardware, particularly processors with many cores. It is much better to let the well-tested low-level libraries handle parallelism rather than implementing messy multiprocessing/multithreading python code when possible. For example, let's compare the time required to calculating FFTs for each trace in a large stream vs doing it all at once on a DataArray created with obsplus, both of which should return the same result. The latter will be more efficient because it allows numpy to better plan optimization strategies, as well as avoid python loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "print(f'numpy version: {np.__version__}')\n",
    "print(f'xarray version: {xr.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test streams with random data\n",
    "import numpy as np\n",
    "import obspy\n",
    "\n",
    "\n",
    "num_stations = 200\n",
    "sr = 100\n",
    "data_length = sr * 60\n",
    "\n",
    "traces = []\n",
    "for station in ('{x:03d}'.format(x=x) for x in range(num_stations)):\n",
    "    data = np.random.rand(data_length)\n",
    "    stats = dict(network='OP', station=station, location='', channel='HHZ',\n",
    "                 sampling_rate=sr)\n",
    "    traces.append(obspy.Trace(data=data, header=stats))\n",
    "\n",
    "st = obspy.Stream(traces=traces)\n",
    "print(st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to data array\n",
    "dar = obspy_to_array(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Time looping through traces and performing fft\n",
    "out1 = np.array([np.fft.rfft(tr.data) for tr in st])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Time performing fft in one-go on large numpy block\n",
    "out2 = np.fft.rfft(dar.data, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after flattening one dimension in out2, the results should be (nearly) the same\n",
    "assert np.allclose(out1, out2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The xarray version is usually between 2 and 20 times faster, depending on the number of cores in your CPU and the python distribution you are using.\n",
    "\n",
    "Moreover, as mentioned above, xarray provides ways of easily working with dask for distributed computing. This would be a bit more difficult using `Stream`s. See [this](http://xarray.pydata.org/en/stable/dask.html) for more details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organization\n",
    "\n",
    "With the data organized in a 3D cube of sorts, it becomes fairly natural to slice and manipulate the data because xarray, like pandas, has intuitive and efficient indexes which allow sensible broadcasting. Here are a few examples of what you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get middle 10 seconds of data\n",
    "time_mean = dar.time.mean()\n",
    "duration = dar.time.max() - dar.time.min()\n",
    "dar.sel(time=slice(time_mean - 5, time_mean + 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim seed_id (channels) to only include data from every 13th station\n",
    "stations = list('OP.{x:03d}..HHZ'.format(x=x) for x in range(0, data_length, 13))\n",
    "dar.where(dar.seed_id.isin(stations), drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple detrend using mean\n",
    "dar - dar.mean(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rolling sta/lta on each channel\n",
    "sta_samples = 25\n",
    "lta_samples = 200\n",
    "\n",
    "sta = dar.rolling(time=sta_samples).mean()\n",
    "lta = dar.rolling(time=lta_samples).mean()\n",
    "\n",
    "result = sta / lta\n",
    "print(result.dropna(dim='time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obsplus Accessor methods\n",
    "Obsplus registers an [xarray accessor](http://xarray.pydata.org/en/stable/internals.html#extending-xarray) to add seismic specific functionality. These are accessed via the `ops` attribute which is available as long as obsplus is imported. Here is a brief tour of some of the methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a data array from the bingham dataset\n",
    "import obsplus\n",
    "fetcher = obsplus.load_dataset('bingham').get_fetcher()\n",
    "# init dict of {event_id: stream} and get catalog/inventory\n",
    "st_dict = dict(fetcher.yield_event_waveforms(time_before=5, time_after=30))\n",
    "cat = fetcher.event_client.get_events()\n",
    "inv = fetcher.station_client.get_stations()\n",
    "# create datarray\n",
    "dar = obsplus.obspy_to_array(st_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select channels based on seed_ids (wildcards permitted)\n",
    "filtered_dar = dar.ops.sel_sid('UU.*.*.ENZ')\n",
    "\n",
    "# sanity checks\n",
    "assert len(filtered_dar.seed_id)\n",
    "for seed_id in filtered_dar.seed_id.values:\n",
    "    assert seed_id.endswith('ENZ')\n",
    "    assert seed_id.startswith('UU')\n",
    "\n",
    "print(filtered_dar.seed_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcule rffts (note the \"time\" dimension has changed to frequency)\n",
    "rfft = dar.ops.rfft()\n",
    "# print dimensions and corresponding size\n",
    "print(dict(zip(rfft.dims, rfft.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate inverse rffts\n",
    "irfft = rfft.ops.irfft()\n",
    "# print dimensions and corresponding size\n",
    "print(dict(zip(irfft.dims, irfft.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back into a dict of streams\n",
    "st_dict = dar.ops.to_stream()\n",
    "for event_id, st in st_dict.items():\n",
    "    print(f'event_id: {event_id} stream_size: {len(st)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach event information\n",
    "dar_with_events = dar.ops.attach_events(cat)\n",
    "# note the extra coords that have now been attached\n",
    "print(dar_with_events.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim all waveforms to the mean of the P times picked on the same station\n",
    "# if no such P pick exists the channels will not be trimmed\n",
    "out = dar_with_events.ops.trim('p_time', aggregate_by='station')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over slices of stations\n",
    "for seed_dar in dar.ops.iter_seed('station'):\n",
    "    print(f'got channels: {set(seed_dar.seed_id.values)} in yielded data array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the norm of the data recorded at each station\n",
    "from numpy.linalg import norm\n",
    "dar.ops.agg(np.linalg.norm, level='station')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
