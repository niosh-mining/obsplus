{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Waveforms to Xarray\n",
        "\n",
        "\u003cdiv class\u003d\"alert alert-warning\"\u003e\n",
        "\n",
        "**Warning**: This part of obsplus is still very experimental and subject to rapid changes, proceed with caution.\n",
        "\n",
        "\u003c/div\u003e\n",
        "\n",
        "\n",
        "The [xarray library](http://xarray.pydata.org/en/stable/) offers pandas-like data structures that are not limited to 2 dimensions (rows and columns). We have found working with seismic waveform data in such a way can be useful, but certainly is not as general as [obspy streams](https://docs.obspy.org/packages/autogen/obspy.core.stream.Stream.html). Particularly, Xarray data structures don\u0027t work well with gappy data or data with non-uniform sampling rates.\n",
        "\n",
        "Before attempting to use these features in obsplus we highly recommend you read through the [xarray documentation](http://xarray.pydata.org/en/stable/) as the API may take a bit of time to learn. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Creating data arrays\n",
        "\n",
        "Creating DataArrays from ObsPy objects is straight-forward. A single `Trace`, `Stream`, or collection (list-like) of either, or a mapping (dict-like) of either are all valid inputs. If a mapping is used, they keys will often correspond to an event id.\n",
        "\n",
        "Conceptually the DataArray looks like this:\n",
        "\n",
        "\u003cimg src\u003d\"../../images/data_array2.png\" width\u003d\"350\" height\u003d\"350\" align\u003d\"left\"/\u003e\n",
        "\u003cbr clear\u003d\"all\"\u003e\n",
        "\n",
        "Each `DataArray` instance created by ObsPlus has three dimensions:\n",
        "\n",
        "1. __stream_id__: The keys used in the dictionary or an integer starting at 0.\n",
        "\n",
        "2. __seed_id__: The seed id (ie network.station.location.channel) of each trace.\n",
        "\n",
        "3. __time__: Floating point values beginning at zero and incrementing by the sampling period.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import obspy\n",
        "import obsplus\n",
        "from obsplus import obspy_to_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "st \u003d obspy.read()\n",
        "# create data array from a trace\n",
        "from_trace \u003d obsplus.obspy_to_array(st[0])\n",
        "# create data array from stream\n",
        "from_stream \u003d obsplus.obspy_to_array(st)\n",
        "# create a data array from a list of streams\n",
        "st_list \u003d [st.copy() for _ in range(3)]\n",
        "from_list \u003d obsplus.obspy_to_array(st_list)\n",
        "# create data array from a dict of streams\n",
        "st_dict \u003d {f\u0027event{x}\u0027: st.copy() for x in range(3)}\n",
        "from_dict \u003d obsplus.obspy_to_array(st_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "print(from_trace.dims)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The data array created from a single trace will only have a size of one in the stream_id and seed_id columns, and the time dimension will be as long as the trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "print(from_trace.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The dataarray created from a dict of streams, however, will be larger:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "print(from_dict.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The xarray `str` representation is fairly large, but very descriptive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# print value for the seed_id dimension\n",
        "print(from_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "DataArrays can also be converted back to dictionaries of `Stream` objects. The transformation *should* be lossless:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "print(from_dict.ops.to_stream())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Advantages of the DataArray\n",
        "\n",
        "The DataArray has two potential advantages over the Stream representation:\n",
        "\n",
        "   1. Efficiency \n",
        "    \n",
        "   2. Organization\n",
        "\n",
        "### Efficiency\n",
        "\n",
        "There have been many efforts to improve efficiency of numpy/scipy functionality. Some of these, such as [Intel\u0027s MKL](https://software.intel.com/en-us/mkl) ship with scientific python distributions, like [Anaconda](https://www.anaconda.com/). These optimizations are great because you don\u0027t need to change anything about your code; it just runs faster.\n",
        "\n",
        "Some of these optimizations involve making better use of modern hardware, particularly processors with many cores. It is much better to let the well-tested low-level libraries handle parallelism rather than implementing messy multiprocessing/multithreading python code when possible. For example, let\u0027s compare the time required to calculating FFTs for each `Trace` in a large `Stream` vs doing it all at once on a `DataArray` created with ObsPlus, both of which should return the same result. The latter will be more efficient because it allows numpy to better plan optimization strategies, as well as avoids python loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import xarray as xr\n",
        "\n",
        "print(f\u0027numpy version: {np.__version__}\u0027)\n",
        "print(f\u0027xarray version: {xr.__version__}\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# create test streams with random data\n",
        "import numpy as np\n",
        "import obspy\n",
        "\n",
        "\n",
        "num_stations \u003d 200\n",
        "sr \u003d 100\n",
        "data_length \u003d sr * 60\n",
        "\n",
        "traces \u003d []\n",
        "for station in (\u0027{x:03d}\u0027.format(x\u003dx) for x in range(num_stations)):\n",
        "    data \u003d np.random.rand(data_length)\n",
        "    stats \u003d dict(network\u003d\u0027OP\u0027, station\u003dstation, location\u003d\u0027\u0027, channel\u003d\u0027HHZ\u0027,\n",
        "                 sampling_rate\u003dsr)\n",
        "    traces.append(obspy.Trace(data\u003ddata, header\u003dstats))\n",
        "\n",
        "st \u003d obspy.Stream(traces\u003dtraces)\n",
        "print(st)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# convert to data array\n",
        "dar \u003d obspy_to_array(st)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Time looping through traces and performing fft\n",
        "out1 \u003d np.array([np.fft.rfft(tr.data) for tr in st])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Time performing fft in one-go on large numpy block\n",
        "out2 \u003d np.fft.rfft(dar.data, axis\u003d-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "# after flattening one dimension in out2, the results should be (nearly) the same\nout1 \u003d np.array([np.fft.rfft(tr.data) for tr in st])\nout2 \u003d np.fft.rfft(dar.data, axis\u003d-1)\nassert np.allclose(out1, out2[0])"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The xarray version is usually between 2 and 20 times faster, depending on the number of cores in your CPU and the python distribution you are using. However, this notebook may not show much of a difference if it was executed on the ReadTheDocs server. The best way to assess performance gains is to download and run this notebook yourself.\n",
        "\n",
        "Moreover xarray provides ways of easily working with dask for distributed computing. This would be a bit more difficult using `Stream`s. See [this](http://xarray.pydata.org/en/stable/dask.html) for more details. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Organization\n",
        "\n",
        "With the data organized in a 3D cube of sorts, it becomes fairly natural to slice and manipulate the data because xarray, like pandas, has intuitive and efficient indexing and sensible broadcasting. Here are a few examples of what you can do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# get middle 10 seconds of data\n",
        "time_mean \u003d dar.time.mean()\n",
        "duration \u003d dar.time.max() - dar.time.min()\n",
        "dar.sel(time\u003dslice(time_mean - 5, time_mean + 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Trim seed_id (channels) to only include data from every 13th station\n",
        "stations \u003d list(\u0027OP.{x:03d}..HHZ\u0027.format(x\u003dx) for x in range(0, data_length, 13))\n",
        "dar.where(dar.seed_id.isin(stations), drop\u003dTrue)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# simple detrend using mean\n",
        "dar - dar.mean(dim\u003d\u0027time\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# calculate rolling sta/lta on each channel\n",
        "sta_samples \u003d 25\n",
        "lta_samples \u003d 200\n",
        "\n",
        "sta \u003d dar.rolling(time\u003dsta_samples).mean()\n",
        "lta \u003d dar.rolling(time\u003dlta_samples).mean()\n",
        "\n",
        "result \u003d sta / lta\n",
        "print(result.dropna(dim\u003d\u0027time\u0027))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Obsplus Accessor methods\n",
        "Obsplus registers an [xarray accessor](http://xarray.pydata.org/en/stable/internals.html#extending-xarray) to add seismic specific functionality. These are accessed via the `ops` attribute which is available as long as obsplus is imported. Here is a brief tour of some of the methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# build a data array from the crandall dataset\n",
        "import obsplus\n",
        "ds \u003d obsplus.load_dataset(\u0027crandall\u0027)\n",
        "# get catalog, inventory and fetcher\n",
        "cat \u003d ds.event_client.get_events()\n",
        "inv \u003d ds.station_client.get_stations(network\u003d\u0027TA\u0027)\n",
        "fetcher \u003d ds.get_fetcher()\n",
        "# init dict of {event_id: stream} and get catalog/inventory\n",
        "st_dict \u003d dict(fetcher.yield_event_waveforms(time_before\u003d5, time_after\u003d30))\n",
        "# create datarray\n",
        "dar \u003d obsplus.obspy_to_array_dict(st_dict)[40]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "print(cat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# select channels based on seed_ids (wildcards permitted)\n",
        "filtered_dar \u003d dar.ops.sel_sid(\u0027TA.*.*.BHZ\u0027)\n",
        "\n",
        "# filter check\n",
        "assert len(filtered_dar.seed_id)\n",
        "for seed_id in filtered_dar.seed_id.values:\n",
        "    assert seed_id.endswith(\u0027BHZ\u0027)\n",
        "    assert seed_id.startswith(\u0027TA\u0027)\n",
        "\n",
        "print(filtered_dar.seed_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Calcule rffts (note the \"time\" dimension has changed to \"frequency\")\n",
        "rfft \u003d dar.ops.rfft()\n",
        "# print dimensions and corresponding size\n",
        "print(dict(zip(rfft.dims, rfft.shape)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Calculate irffts\n",
        "irfft \u003d rfft.ops.irfft()\n",
        "# print dimensions and corresponding size\n",
        "print(dict(zip(irfft.dims, irfft.shape)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# convert back into a dict of streams\n",
        "st_dict \u003d dar.ops.to_stream()\n",
        "for event_id, st in st_dict.items():\n",
        "    print(f\u0027event_id: {event_id} stream_size: {len(st)}\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# attach event information\n",
        "dar_with_events \u003d dar.ops.attach_events(cat)\n",
        "# note the extra coords that have now been attached\n",
        "print(dar_with_events.coords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# trim all waveforms to the mean of the P times picked on the same station\n",
        "# if no such P pick exists the channels will not be trimmed\n",
        "out \u003d dar_with_events.ops.trim(\u0027p_time\u0027, aggregate_by\u003d\u0027station\u0027)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# iterate over slices of stations\n",
        "for seed_dar in dar.ops.iter_seed(\u0027station\u0027):\n",
        "    print(f\u0027got channels: {set(seed_dar.seed_id.values)} in yielded data array\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# calculate the norm of the data recorded at each station\n",
        "from numpy.linalg import norm\n",
        "dar.ops.agg(np.linalg.norm, level\u003d\u0027station\u0027)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}